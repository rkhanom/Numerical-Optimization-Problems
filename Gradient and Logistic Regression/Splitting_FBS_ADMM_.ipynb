{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgAmkICWV2t1"
      },
      "source": [
        "Splitting Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJAC--A6V2t1"
      },
      "outputs": [],
      "source": [
        "from logreg import create_classification_problem\n",
        "import numpy as np\n",
        "from numpy import sqrt, sum, abs, sign, max, maximum, minimum, logspace, exp, log, log10, zeros\n",
        "from numpy.linalg import norm\n",
        "from numpy.random import randn, rand, normal, randint\n",
        "import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(0)\n",
        "def good_job(path):\n",
        "    a = plt.imread(urllib.request.urlopen(path))\n",
        "    fig = plt.imshow(a)\n",
        "    fig.axes.get_xaxis().set_visible(False)\n",
        "    fig.axes.get_yaxis().set_visible(False)\n",
        "    plt.show()\n",
        "np.random.seed(0)\n",
        "\n",
        "# A utility function for estimating Lipschitz constants\n",
        "def estimate_lipschitz(g, x):\n",
        "    # Your work here\n",
        "    y = x+normal(size=x.shape)\n",
        "    L = norm(g(x)-g(y))/norm(x-y)\n",
        "    return L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmk2saCSV2t1"
      },
      "source": [
        "# Problem 1: FBS\n",
        "Write a method that uses FBS to solve a general problem of the form\n",
        "$$\\min \\quad f(x)+g(x),$$\n",
        "where $f$ is smooth and $g$ is simple.\n",
        "You can do this by building on your gradient descent code from homework 2.  \n",
        "The argument $f$ computes the scalar-valued function $f(x).$  The argument `gradf` is a function handle that computes the gradient of $f.$  This means that\n",
        " $$ \\mathtt{gradf}(x) = \\nabla f(x).$$\n",
        "  The argument `proxg` is a function handle that computes the proximal operator of $g$ with stepsize $\\tau.$  This means that\n",
        "  $$proxg(z,\\tau) = \\arg \\min_x  g(x) + \\frac{1}{2\\tau}\\|x-z\\|^2.$$\n",
        "  Your method should start by estimating the initial stepsize $\\tau$ using the Lipschitz constant for the gradient of $f.$  You already did this in homework 5 in your gradient descent method. The method should then perform an iteration of FBS, and use a backtracking line search until the following condition is met:\n",
        "      $$f(x^{k+1}) \\le f(x^k)+\\langle x^{k+1}-x^k, \\nabla f(x^k) \\rangle + \\frac{1}{2\\tau}\\|x^k-x^{k+1}\\|^2.$$\n",
        "\n",
        "Your method should terminate when the residual\n",
        "   $\\frac{1}{\\tau}\\|x^{k+1}-x^k\\|$\n",
        "   is \"small\" according to some reasonable criteria.  Formulas for the line search and residuals can be found in the paper \"A field guide to forward backward splitting with a FASTA implementation,\" (the line search condition is discussed in section 4.4, and formulas for the residuals are in section 4.6)  and also in the lecture slides.\n",
        "   \n",
        "  Your method should return an array containing the solution to the problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_lipschitz(g, x):\n",
        "    # Your work here\n",
        "    y = x + np.random.normal(size=x.shape)\n",
        "    L = np.linalg.norm(g(x) - g(y))/ np.linalg.norm(x-y)\n",
        "    return L"
      ],
      "metadata": {
        "id": "Nx0M9GQNWxM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def  fbs(f, gradf, proxg, x0, max_iters = 10000, tol=1e-6):\n",
        "    ## Your work here!\n",
        "    converged = False\n",
        "    step_size = 10 / estimate_lipschitz(gradf,x0)\n",
        "    x = x0\n",
        "    while(max_iters and not converged):\n",
        "        max_iters -=1\n",
        "        gradient = gradf(x)\n",
        "        x_n = proxg(x - step_size * gradient, step_size)\n",
        "        while(f(x_n) > (f(x) + np.vdot(x_n - x, gradient) + (0.5/step_size)*(np.linalg.norm(x_n - x))**2 )):\n",
        "            step_size /= 2.\n",
        "            x_n = proxg(x - step_size * gradient, step_size)\n",
        "\n",
        "        if(np.linalg.norm(x_n - x)/step_size < tol):\n",
        "            converged = True\n",
        "\n",
        "        x = x_n\n",
        "    return x"
      ],
      "metadata": {
        "id": "yp86tdb7XBsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQwZM1xcV2t2"
      },
      "source": [
        "### Now, run this unit test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btt2ZWwIV2t2",
        "outputId": "d705343a-36a4-4f38-99b8-3578dba8aebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your FBS solver worked!\n"
          ]
        }
      ],
      "source": [
        "# Minimizer f+g where f(x)=.5(x+2)^2, g(x)=.5(x)^2\n",
        "f = lambda x: 0.5*(x-2)**2\n",
        "gradf = lambda x: x-2\n",
        "g = lambda x: 0.5*x**2\n",
        "proxg = lambda z,t: z/(1+t)\n",
        "x0 = np.array(5)\n",
        "\n",
        "x = fbs(f,gradf,proxg,x0)\n",
        "assert abs(x-1)<1e-4, \"Your solution is not accurate enough!\"\n",
        "print('Your FBS solver worked!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS2D-y3fV2t2"
      },
      "source": [
        "### Now let's cook up a sparse least squares problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysYdqFLeV2t2"
      },
      "outputs": [],
      "source": [
        "# Don't modify this block!\n",
        "A = randn(100,200)\n",
        "x_true = zeros((200,1))\n",
        "x_true[0:10] = 1\n",
        "b = A@x_true\n",
        "mu = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO6W1J10V2t2"
      },
      "source": [
        "### Use your FBS solver to solve the problem\n",
        "$$\\arg \\min \\quad  \\mu|x| + \\frac{1}{2}\\|Ax-b\\|^2$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def shrink(z,t):\n",
        "    x = sign(z)* np.fmax(abs(z) - t,0);\n",
        "    return x\n",
        "\n",
        "print(shrink(np.array([3,5,2]),5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awM0dPQeXPo5",
        "outputId": "87795fd2-1a65-45d4-c556-b39224f472ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill out the implementations of these methods. I suggest using lambda functions to keep it short\n",
        "f = lambda x: 0.5*np.linalg.norm(A@x-b)**2 # your work here\n",
        "gradf = lambda x: A.T@(A@x - b) # your work here\n",
        "g = lambda x: mu * np.linalg.norm(x,ord=1) # your work here\n",
        "proxg = lambda z,t: shrink(z,mu*t) # your work here"
      ],
      "metadata": {
        "id": "dW0mXYlXXS-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwfbpa_-V2t2"
      },
      "source": [
        "### Now, run this method to call the solver and test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MKi6gFpV2t2",
        "outputId": "46bc6cd8-eca6-4d51-a8cb-c4da528c5da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST PASSED! Wow! You are so in the zone right now!\n"
          ]
        }
      ],
      "source": [
        "# Do not modify this block!\n",
        "x0 = zeros((200,1))\n",
        "x = fbs(f,gradf,proxg,x0, tol=1e-8) # Note: I use super high accuracy here so that my unit best below works\n",
        "\n",
        "# Test that your solution satisfies the optimality condition for the problem\n",
        "final_grad = gradf(x)\n",
        "assert max(abs(final_grad[x==0])) <= mu+1e-4, \"Your solution is incorrect\"\n",
        "assert norm(final_grad[x!=0] + mu*sign(x[x!=0]))/norm(final_grad[x!=0]) < 1e-4,  \"Your solution is incorrect\"\n",
        "print('TEST PASSED! Wow! You are so in the zone right now!')\n",
        "#good_job(\"https://www.cs.umd.edu/~tomg/img/important_memes/otter.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fONzi1rAV2t2"
      },
      "source": [
        "# Problem 2: ADMM\n",
        "In this problem,\n",
        "you'll write an \"unwrapped ADMM\" solver for the support-vector machine problem\n",
        "$$\n",
        " \\min \\quad \\frac{1}{2} \\|x\\|^2+Ch(Ax)\n",
        "$$\n",
        "where $h(z) = \\sum_i \\max\\{1-z_i,0\\}$ is the hinge loss function, and  $A = YX$ is the product of the (diagonal) label matrix with the data matrix.  Your solver will be based on the constrained formulation\n",
        "\n",
        "\\begin{align}\n",
        " \\min &\\quad \\frac{1}{2} \\|x\\|^2+Ch(y)\\\\\n",
        " \\text{subject to}&\\quad   y-Ax=0.\n",
        "\\end{align}\n",
        "\n",
        "Start by running the block below to produce a test problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qItG26hCV2t2"
      },
      "outputs": [],
      "source": [
        "# Define a classification problem\n",
        "X, y = create_classification_problem(100, 10, cond_number=10)\n",
        "A = y*X\n",
        "t = 1/norm(A.T@A)\n",
        "C = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOYZ6VKHV2t3"
      },
      "source": [
        "### Write the scaled augmented Lagrangian for the constrained problem\n",
        " Use $\\lambda$ to donote the Lagrange multiplier.\n",
        "\n",
        " **Your solution here**\n",
        "\n",
        " $$L_r(x,y,\\lambda)=\\frac {1}{2}{\\|x\\|}^2+Ch(y)+\\frac{\\tau}{2}{\\|Ax-y+\\lambda\\|}^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C5Vz4igV2t3"
      },
      "source": [
        "### Write the system of equations that needs to be solved to update $x$\n",
        " **Your solution here**\n",
        "  $$x^{k+1}= {arg min}_x\\frac {1}{2}{\\|x\\|}^2+\\frac{\\tau}{2}{\\|Ax-y+\\lambda\\|}^2$$\n",
        " $$(I+\\tau A^TA)x^{k+1}= \\tau A^T (y^{k+1}-\\lambda^k)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke5AW7sQV2t3"
      },
      "source": [
        "###Â Write a routine to evaluate the prox operator of the hinge loss. Your routine will return the solution to the prox problem\n",
        "$$\\text{prox}_h(z,t) = \\arg\\min_x h(x) + \\frac{1}{2t}\\|x-z\\|^2.$$\n",
        "You cannot use an iterative method to compute this value.  Note: this can be done with one line of code.  The prox operator for the hinge loss is a lot like the prox operator for L1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcsYKXALV2t3"
      },
      "outputs": [],
      "source": [
        "def hprox(z,t):\n",
        "    # Your work here\n",
        "    x=z+np.maximum(np.minimum(1-z,t),0)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqOTqNXWV2t3"
      },
      "source": [
        "### Now run these unit tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAFB96QaV2t3"
      },
      "outputs": [],
      "source": [
        "assert hprox(0,1)==1, \"Your prox operator failed unit test 1\"\n",
        "assert hprox(1,1)==1, \"Your prox operator failed unit test 2\"\n",
        "assert hprox(-1,1)==0, \"Your prox operator failed unit test 3\"\n",
        "assert hprox(1,2)==1, \"Your prox operator failed unit test 4\"\n",
        "assert hprox(1,3)==1, \"Your prox operator failed unit test 5\"\n",
        "assert hprox(4,3)==4, \"Your prox operator failed unit test 6\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqSz0ViKV2t3"
      },
      "source": [
        "### Write an ADMM loop to solve the SVM problem above.  \n",
        "Use your prox operator for the $y$ update.\n",
        "Run the solver until the primal and dual residuals satisfy\n",
        "$$p^k \\le 10^{-5} \\max_{i<k}p^i$$\n",
        "$$d^k \\le 10^{-5} \\max_{i<k}d^i.$$\n",
        "The residuals are given by\n",
        "$$p^k = \\|Ax^k-y^k\\|$$\n",
        "$$d^k = \\|\\tau A^T(y^k-y^{k-1})\\|.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj1jLGLUV2t3"
      },
      "outputs": [],
      "source": [
        "x = zeros((10,1))\n",
        "y = zeros((100,1))\n",
        "l = zeros((100,1))\n",
        "\n",
        "# Your work below\n",
        "p_list=[]\n",
        "d_list=[]\n",
        "p_max=0\n",
        "d_max=0\n",
        "\n",
        "while True:\n",
        "    x_new= t*A.T@(A@x-y-l)\n",
        "    y_new=hprox(A@x_new+l,C/t)\n",
        "    l_new=l-y_new+A@x_new\n",
        "    p=np.linalg.norm(A@x_new-y_new)\n",
        "    d=np.linalg.norm(t*A.T@(y_new-y))\n",
        "    if(p<=p_max*10**(-5)and d<= d_max*10**(-5)):\n",
        "        break\n",
        "    p_list.append(p)\n",
        "    d_list.append(d)\n",
        "    p_max=np.max(p_list)\n",
        "    d_max=np.max(d_list)\n",
        "    x=x_new\n",
        "    l=l_new\n",
        "    y=y_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvTjk_CRV2t3"
      },
      "source": [
        "### Now run this unit test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdprgbguV2t3",
        "outputId": "bd995de3-3107-4fc3-dc29-d66bec3e993a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Congrats! Your solver works!!\n"
          ]
        }
      ],
      "source": [
        "# There are two different optimality conditions that could be satisfied depending on how you formulate the lagrangian\n",
        "error1 = norm(x+t*A.T@(A@x-y-l))\n",
        "error2 = norm(x+t*A.T@(A@x-y+l))\n",
        "assert min(error1/norm(x), error2/norm(x))<1e-3, 'Your ADMM solver did not produce an accurate solution.'\n",
        "z1 = A@x-l\n",
        "z2 = A@x+l\n",
        "error1 = norm(y-z1 - maximum(minimum(1-z1,C/t),0))\n",
        "error2 = norm(y-z2 - maximum(minimum(1-z2,C/t),0))\n",
        "assert min(error1/norm(y),error2/norm(y)) <1e-3, 'Your ADMM solver did not produce an accurate solution.'\n",
        "print('Congrats! Your solver works!!')\n",
        "#good_job(\"https://www.cs.umd.edu/~tomg/img/important_memes/ron_awesome.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}